
<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>TexanTurnout - Predicting Voter Turnout in Texas Elections</title>

    <!-- Bootstrap Core CSS -->
    <!-- <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet"> -->
    <link href="bootstrap.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="font-awesome-4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">

    <!-- Theme CSS -->
    <link href="grayscale.css" rel="stylesheet">
    <!-- <link href="css/grayscale.min.css" rel="stylesheet"> -->

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

    <!-- Navigation -->
    <nav class="navbar navbar-custom navbar-fixed-top" role="navigation">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
                    Menu <i class="fa fa-bars"></i>
                </button>
                <a class="navbar-brand page-scroll" href="#page-top">
                    <span class="light">TexanTurnout</span> Home
                </a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-right navbar-main-collapse">
                <ul class="nav navbar-nav">
                    <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                    <li class="hidden">
                        <a href="#page-top"></a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#motivation">Motivation</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#methodology">Methodology</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#analysis">Analysis</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#results">Results</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#contact">Contact</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

	  <!-- About Section -->
    <section id="about" class="container content-section text-center">
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
<h2 style="font-size: 45px"> TexanTurnout</h2>
	      
	      <center><h3>Predicting Voter Turnout Rates in Texas Elections</h3>
	      <a href="#motivation" class="btn btn-circle page-scroll">
                            <i class="fa fa-angle-double-down animated"></i>
                        </a></center>
	      </div>
	      </div>
	      </section>


    <!-- About Section -->
    <section id="motivation" class="container content-section text-center">
        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h2>Problem Motivation</h2>

                <p style="text-indent: 3em">
                Any sort of predictive election analysis requires an estimate for the fraction of registered voters
	      which will actually go to the polls and cast a vote. This is traditionally done with pre-election polling. However, this can be expensive and traditional phone-based polling methods
                </p>
                
                <p>
	      I participated in the Insight Data Science Program in the Fall of 2017. As part of Insight, I worked with the company Value Voting to help improve their prediction 
                </p>

                <p>
                In this post, I will describe the work I did with Value Voting to help improve their methodology to predict overall voter turnout rates. In particular, I will highlight two different approaches to this challenge: one is based on the individual level, where I predict whether a given person or not will vote based on demographics and past voting history. In a complementary approach, I take a different view by examining what factors particular to a given election may cause the overall turnout rate to change.
                </p>



            </div>
        </div>
    </section>

    <!-- Approach Section -->
    <section id="methodology" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-14 col-lg-offset-0">
                <h2>Methodology</h2>
                <p style="text-indent: 3em">
                Robotec's IR camera generates 320x256 pixel monochromatic images with 16-bit depth such as those pictured below. They provided to me 1000 images, 287 annotated with the bounding boxes surrounding humans. Their initial approach to human detection was to use a threshold-selective naive blob detection method such as that implemented in the opencv library (SimpleBlobDetector). Threshold selection presents several complications. There may be features that are brighter than humans so brightness alone is insufficient. Furthermore ambient lighting conditions will vary especially considering the robot's mobility. 
                </p>

                <p style="text-align: center">
                <img src="./img/flir_human_center.jpg", class="border"> &emsp;&emsp;
                <img src="./img/flir_photocopier.jpg", class="border">  &emsp;&emsp;
                <img src="./img/flir_human_distant.jpg", class="border">
                </p>

                <p>
                To overcome those issues you could apply a catch-all threshold. While such an approach potentially recalls all human features it also captures all features that fall within a given intensity/contrast window such as lights, and electronic equipment, as exemplified above. This severly boosts false positive detections and esentially leaves all the work ahead of us whereby the discrimination of humans in those detected features is still required. Again the variability of the human subjects is tremendous, where they may be in a range of field depths, in a variety of poses, or partially obscured by dark objects. As such, feature selection and engineering quickly becomes an overwhelming laundry list with many edge cases. 
                </p>

                <p>
                There are several approaches to avoiding such feature engineering and selection. One is to use a supervised <a href="https://en.wikipedia.org/wiki/Support_vector_machine">support vector machine</a> (SVM) initialized with a <a href="https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients#SVM_classifier">histogram of oriented gradients</a> (HOG) descriptor. OpenCV implements such an approach but a cursory exploration of this method on the labelled data demonstrated poor performance, far below 50% recall and not calculated at a rate close to real-time.
                </p>
                <p>
                An increasingly powerful approach, and that which was applied here, is the convolutional neural network (CNN). CNNs are complicated models with several moving parts. Since this project represents my first foray into CNNs, it is unrealistic to have attained a deep understanding of their implimentation but I nevertheless offer a high-level over view here. Feel free to <a class="page-scroll" href="#darknet">skip over</a> this section to the project specific discussion of CNNs below.

                </p>
                <h3 style="text-align: left">Neural Networks</h3>
                <p style="text-indent: 3em">
                Neural networks (NNs) are inspired by biological neural systems whereby neurons have some probability of firing and causing subsequent activations of nearby neurons. These series of activations combine to represent a particular state. Different combinations of neuron activations lead to unique states. Multiple neurons may also combine to activate a single neuron. A NN similarly consists of nodes (neurons) which may be active, inactive, or somewhere in between and these nodes are arranged into layers. In the most simple case, 3 layers are required: an input layer, a hidden layer, and an output layer. The input layer will have a number of nodes representing its input data. In the case of image detection these input nodes are usually the pixels. The hidden layer nodes are each connected to every input node and every output node. How the hidden layer responds to the input layer may be modified by weighting its contribution to the subsequent activiations (weights) and setting its condition for firing (bias). In this way we may "train" the hidden layer by modifying its weights and biases such that a series of unique inputs will activate a specific output node reprsenting a unqiue feature. NNs are therefore a form of supervised learning. If we train a layer to respond to a known input to generate a given output then novel inputs which lead to the same output have some probability of being the same as the input for which the hidden layer was trained. By adding more hidden layers and more nodes to each layer, highly complex internodal relationships may be established for predicting the probable content any number of inputs.
                </p>

                <h3 style="text-align: left">Convolutional Neural Networks</h3>
                <p style="text-indent: 3em">
                Because of the complete connectivity between nodes in a NN, respresenting complex inputs requires fitting many thousands of weights. Considering a 100x100 pixel image with 3 color channels, it would be necessary to fit 30000 (100x100x3) weights for the detection of a single feature, more feature sensitivity would compound parameters. Furthermore, this network could only be used with equally-sized images. Not very useful in a world where our cell phones routinely record 10 million pixel images for example. Furthermore, fitting to a parameter space of this scale commonly results in over-fitting. 
                </p>
                <p>
                Convolutional neural networks (CNNs) solve this problem by reducing the number of fit parameters by reducing neuron interconnectivity. This is achieved by several means. The neurons are arranged into 3 dimensions width, height, and depth. Each layer of the network transforms the pixel volumes into 3D neuron activations where the depth dimension represents the number of spacial filters which are trained on particular image features. Importantly each neural activation volume is only connected to a small region of the previous layer. By reducing the interconnectivity between neurons, the parameter space is significantly reduced. Finally, the last layer computes a class score for some region where the class represents a particular feature in a given sub-region, and this reduces the pixel volume dimensionality vector to the number of classes. 
                </p>
                <p>
                Without delving into a full description of the nature of the layers in a CNN, typical networks invoke 5 types of layers: input; convolutional; activation; pooling; and the fully connected layer. Most distinct to CNNs are the convolutional (conv) and pooling layers (pool). Conv layers consist of a number filters which, in the case of images, span a subsection of the image. Each filter is passed over each subsection of the image but uses the same weights and biases, dramitically reducing fit paramters. Each filter is typically trained on a specfic feature in an image such as an edge, or color blob. Pool layers further reduce dimensionality by downsampling the filters. The combination and number of the layers used is part of network design and can be designed to suit specific applications.  For an introductory discussion of CNNs, with rich visualization aids, see Andrej Karpathy's excellent <a href="http://cs231n.github.io/convolutional-networks/#overview">discussion</a> of the subject.

                </p>
                <p id="darknet"></p>
                <h3  style="text-align: left">Darknet (You Only Look Once)</h3>
                <p style="text-indent: 3em">
                There are a myriad of libraries available that implement CNNs, each with their various strengths and weaknesses. In this case I selected the open source <a href="http://pjreddie.com/darknet/">Darknet</a> CNN and in particular the You Only Look Once (<a href="http://pjreddie.com/darknet/yolo/">YOLO</a>) implementation. Darknet has several advantages. First, and perhaps foremost, it has already been used by Robotec in their visible cameras and therefore will compliment their robot's existing process pipeline. Another relevant feature of Darknet is its speed, comfortably achieving 40 FPS with a mean average precision of 79%. This speed is due partly to its use of the Nvidia <a href="https://developer.nvidia.com/cudnn">cudnn</a> library which exploits the robot's onboard GPU and is specifically designed for increasing the efficiency of CNN calculations. Also crucially, networks may may be retrainined with new data augmenting the features already known to the network. This is especially the case here where many features of humans that a CNN might find, such as certain combinations of edges, will be common to color and heat map human images.    
                </p>

            </div>
        </div>
    </section>

    <!-- Analysis Section -->
    <section id="analysis" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-14 col-lg-offset-0">
                <h2>Analysis</h2>
                <h3  style="text-align: left">Data Preparation</h3>
                <p style="text-indent: 3em">
                The first step was to prepare the data for training and testing. Robotec provided some 250 images annotated with bounding boxes of humans in the image. As such each image is accompanied by a text file specifiying the bounding boxes for humans features in that image. This resulted in some 270 training features since some images contained several humans. To augment this data I used the <a href="https://github.com/puzzledqs/BBox-Label-Tool">BBox-Label-Tool</a> to annotate the bounding boxes of humans in 150 more images. This brought the labelled data to 350 images. From those I set aside 250 images for training and 100 images for testing. To the testing set I added 100 images containing no humans to balance the data for false positives. The test set therefore consists of 200 images, half with human features, half with none. 
                </p>
                <h3  style="text-align: left">Environment Setup and Darknet Implementation</h3>
                <p style="text-indent: 3em">
                As mentioned above, the robot is housed with a Linux system and built-in GPU. However, because Robotec is still very much in the product development phase, the Robot's resources were tied up with other processes and was often in offline mode. As such I prepared an GPU EC2 instance on Amazon Web Services (AWS) for setting up the training and testing environment. Specifically, I compiled the Nvidia cudnn and Darknet libraries.
                </p>
                <p>
                Because Darknet only operates on a single image at a time and generates a common data output file, I prepared a <a href="./scripts/darknet_test.sh">bash script</a> to launch sequential instances of Darknet pointing to a directory containing the test images. The script then moves the result log to a results directory and names it according to the input image name. Therefore for 200 test images, it generates 200 text files indicating the properties of the features found for a given image. Darknet also allows an option to output the test image with the bounding boxes as determined by the network. I combined these output images with <a href="https://ffmpeg.org/">ffmpeg</a> to produce a video of the results embedded below.
                </p>

                <p style="text-align: center">
                <iframe width="800" height="450" src="https://www.youtube-nocookie.com/embed/jN0RL4mPxdY?rel=0&amp;showinfo=0" frameborder="1" allowfullscreen></iframe>
                </p>

                <p>
                Cursory Inspection of the video reveals good performance using the weights pretrained on PASCAL visual object classes (VOC) <a href="http://host.robots.ox.ac.uk/pascal/VOC/">data</a>. While this represents a vast improvement over the naive blob, and SVM HOG descriptor approaches described above, there are clearly some false negatives, false positives, and mis-classifications.   
                </p>
                <h3  style="text-align: left">Determining Accuracy and Recall</h3>
                <p style="text-indent: 3em">
                In order to quantify the efficacy of the model I developed a <a href="./scripts/analyze.txt">python script</a> to inspect the results from Darknet compared with the human-labelled data. Because the human labelled and Darknet bounding box coordinates will have a slight variation it is neccesary to define what counts as a true positive. Similarly the area of matching boxes should be simlar to a defined degree. As such, two box pairs count as a match if they overlap to a specified threshold and have sufficiently similar areas. The later requirement is important to exclude false positives for feature pairs that happen to be fully overlap.
                </p> 

                <p>
                From these definitions the script can determine and report: true and false postives; and true and false negatives, on a per image basis. These per image results may also be summarized to report the standard binary classifier measures  such as sensitivity, accuracy, and z-score. The complete list of analysis returned can be found <a href="https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers">here</a>. Another feature of the script is the ability to display the test images annotated with the human-labelled and Darknet bounding boxes. This can provide a good sanity check on the efficacy of the other analysis.
                </p>
                <h3  style="text-align: left">Results and Retraining</h3>
                <p style="text-indent: 3em">
                The graph below shows the accuracy and recall for the test data using the weights trained on VOC data at various thresholds. The threshold defines which features are outputted based on their classification certainty. The graph indicates a good compromise between accuracy and recall at a threshold of 0.25 with accuracy and recall both near 80%. At lower threholds recall approaches 100% but this severly compromises accuracy.  
                </p>

                <p style="text-align: center">
                <img src="./img/accuracy.png">
                </p>

                <p>
                In order to attempt to boost recall and accuracy attempts were made to retrain the model. Retraining involves some modification of the original C libraries and the last, fully-connected, network layer and programmatic preparation of the input files referencing the training set. With the available training set, these efforts led to no meaningful boost in recall or accuracy. This is likely best explained by two reasons. Firstly a paucity in training data where only 270 features are available represents a very small sample. Secondly, and perhaps more importantly, inspection of the humans features in the training set reveals a vast majority of features for which the model already accurately detected as human. Those features for which the the model failed, such as for obscured humans, or humans in unsual poses (see the images below), represent only a small subsample of the training data. By the very nature of the IR video camera, recording more images is relatively simple and a large data set may be quickly accrued. However, the most time consuming step in preparing these training data is in determining the bounding boxes of human features. Therefore, in order to improve the model I developed a data labelling pipeline for production of these features requiring minimal intervention by the development team.
                </p>

                <p style="text-align: center">
                <img src="./img/flir_sitting.jpg", class="border"> &emsp;&emsp;
                <img src="./img/flir_obscured.jpg", class="border">  &emsp;&emsp;
                <img src="./img/flir_cow.jpg", class="border">
                </p>

                <h3  style="text-align: left">Data Labelling and Retraining Pipeline</h3>
                <p style="text-indent: 3em">
                The two most important qualities for any production line are that it should maximize automation, and it should produce consistent results. Those two requirements laid the grounding principles in my approach to the development of the retraining pipeline. This pipeline, illustrated in the figure below, starts from teh robot and proceeds accordingly: generate training images; store images online; human annotates bounding boxes; store box data online; retrieve box data; retrain with new data; test new model; incorporate new model.
                </p>

                <p style="text-align: center">
                <img src="./img/pipeline.jpg">
                </p>

                <p>
                All of these processes are automated within a single script which itself may eventually be incorporated into the robot's pipeline for full retraining automation. The crucial stages of image upload, bounding box determination are achieved with APIs of <a href="http://cloudinary.com/">Cloudinary</a> and <a href="https://www.scaleapi.com/">Scale</a>, respectively. Bounding box data for each image is stored in a JSON-like object on a <a href="http://heroku.com">Heroku</a> server which may be retrieved by the robot and incorporated into the training cycle. The scripts to achieve this can be viewed here. (<a href="./scripts/autobox.txt">autobox.py</a>, <a href="./scripts/wrap_cloud.txt">wrap_cloud.py</a>, <a href="./scripts/wrap_scale.txt">wrap_scale.py</a> )
                </p>

            </div>
        </div>
    </section>

    <!-- Outcomes Section -->
    <section id="results" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-14 col-lg-offset-0">
                <h2>Results</h2>
                <p>
                Robotec needed a solution to detect humans in these images in real time, and with maximum recall, and accuracy. This had to be an improvement on their naive blob approach. By implementing the Darknet neural network I achieved a recall and accuracy of ~78% while operating at over 30 FPS. This represents a significant improvement over their initial approach. More importantly I have created the tools and techniques to create ongoing improvement and therefore value to the product. In particular I created scripts to automate testing with Darknet, analysis of Darknet's results in comparison with the human-labelled data, and subsequent visualization of these results. Auxialliary to this, I created a retraining pipeline that should vastly increase their data to ensure further improvement in the model. This versatile pipeline is also easily adaptable to other detection systems on the robot and will therefore provide value to many aspects of the product.  
                </p>
            </div>
        </div>
    </section>

    <!-- Contact Section -->
    <section id="contact" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2", style="text-align: center;">
                <h2>Contact Chalence</h2>
                <p>Questions? Comments? Feel free to send me an email</p>
                <p><a href="mailto:csafranek@gmail.com">csafranek@gmail.com</a>
                </p>
                <ul class="list-inline banner-social-buttons">
                    <li>
                        <a href="https://github.com/chalence" class="btn btn-default btn-lg"><i class="fa fa-github fa-fw"></i> <span class="network-name">Github</span></a>
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/chalence" class="btn btn-default btn-lg"><i class="fa fa-linkedin fa-fw"></i> <span class="network-name">LinkedIn</span></a>
                    </li>   
                </ul>
            </div>
        </div>
    </section>

    <!-- Map Section -->
<!--     <div id="map"></div> -->

<!--     <!-- Footer -->
<!--     <footer>
        <div class="container text-center">
        </div>
    </footer> -->

    <!-- jQuery -->
    <script src="vendor/jquery/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="vendor/bootstrap/js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>

    <!-- Google Maps API Key - Use your own API key to enable the map feature. More information on the Google Maps API can be found at https://developers.google.com/maps/ -->
    <!--     <script type="text/javascript" src="https://maps.googleapis.com/maps/api/js?key=AIzaSyCRngKslUGJTlibkQ3FkfTxj3Xss1UlZDA&sensor=false"></script>
     -->
    <!-- Theme JavaScript -->
    <script src="js/grayscale.min.js"></script>

</body>

</html>
